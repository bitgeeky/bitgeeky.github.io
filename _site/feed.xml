<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pankaj Malhotra</title>
    <link>http://pankajmalhotra.com</link>
    <atom:link href="http://pankajmalhotra.com/feed.xml" rel="self" type="application/rss+xml" />
    <description>Pankaj Malhotra aka bitgeeky's blog on Automation, Web QA, Web Development, Algorithms, Coding and his life.</description>
    <language>en-us</language>
    <pubDate>Mon, 08 Jun 2015 03:20:30 +0530</pubDate>
    <lastBuildDate>Mon, 08 Jun 2015 03:20:30 +0530</lastBuildDate>

    
      <item>
        <title>Injecting Javascript In HTML Content Using MITM Proxy</title>
        <link>http://pankajmalhotra.com/Injecting-Javascript-In-HTML-Content-Using-MITM-Proxy</link>
        <pubDate>Mon, 08 Jun 2015 21:43:10 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;Its been a few days that I have been learning about Man In The Middle MITM proxy, my use case was to inject a simple javascript to html pages that users open behind proxy server.&lt;/p&gt;

&lt;p&gt;It actually involves a bit insight on how to do this, and a few small challenges. Here I describe the solutions I found to do this:&lt;/p&gt;

&lt;h2&gt;What is MITM proxy ?&lt;/h2&gt;

&lt;p&gt;An interactive console program that allows traffic flows to be intercepted, inspected, modified and replayed.
So basically it gives the proxy administartor the power to modify any traffic that goes through the proxy. You can play with html content, inject elements, get header data, modify headers, dns spoofing, traffic filteration, redirection and a lot more things you can do with mitmproxy.
&lt;br/&gt;
&lt;a href=&quot;https://github.com/mitmproxy/mitmproxy/tree/master/examples&quot;&gt;Example scripts for doing experimentation&lt;/a&gt;
&lt;br/&gt;
For more information visit &lt;a href=&quot;https://mitmproxy.org/&quot;&gt;mitmproxy official website&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Installation&lt;/h2&gt;

&lt;p&gt;For OS X its distributed as a homebrew package and is really simple to install, just do &lt;code&gt;$ brew install mitmproxy&lt;/code&gt;.
For troubleshooting and setting environment variables have a look at the &lt;a href=&quot;https://mitmproxy.org/doc/install.html&quot;&gt;installation guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Make sure you have all the certificates used by mitm proxy in &lt;code&gt;~/.mitmproxy/&lt;/code&gt;. Install certificates on browser by browsing to &lt;code&gt;mitm.it&lt;/code&gt;. If the traffic is passing through the proxy server this page will show you options to install certificates, just select the platform you are browsing on.  For more information about generating and installing certificates see &lt;a href=&quot;https://mitmproxy.org/doc/certinstall.html&quot;&gt;about cerificates&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Running&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ mitmproxy -p port_number
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The default port on which proxy server runs is &lt;code&gt;8080&lt;/code&gt; but you can specify the &lt;code&gt;port_number&lt;/code&gt; by using &lt;code&gt;-p&lt;/code&gt; flag. It will open up a window showing traffic passing through proxy.&lt;/p&gt;

&lt;p&gt;Configure browser to use proxy server by specifying host as &lt;code&gt;localhost&lt;/code&gt; and the port on which proxy server is listening.
You will start seeing requests though the proxy and traffic passing through it.
&lt;img src=&quot;/public/images/mitmproxy.png&quot; alt=&quot;MITM Console&quot; style=&quot;margin-right:15px; width: 100%; height: 100%&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Modifying traffic passing through proxy&lt;/h2&gt;

&lt;p&gt;You can pass scripts as arguments while running proxy server which will modify the traffic according to the logic you specify in script.
The script will operate on each request passing through proxy, and will make the desired modification.&lt;/p&gt;

&lt;h3&gt;Inline Scripts&lt;/h3&gt;

&lt;p&gt;Basic Structure of an Inline script is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;def on_event(context, flow):
    some_logic
    another_logic
    do_something
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The first argument to each event method is an instance of ScriptContext that lets the script interact with the global mitmproxy state. 
&lt;code&gt;on_event&lt;/code&gt; can be an event like request, response, start, clientconnect, serverconnect or any other event from this &lt;a href=&quot;https://mitmproxy.org/doc/scripting/inlinescripts.html&quot;&gt;list&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Script to inject javascript to html content:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;# Usage: mitmdump -s &amp;quot;js_injector.py src&amp;quot;
# (this script works best with --anticache)
from bs4 import BeautifulSoup
from libmproxy.protocol.http import decoded

# On start of proxy server ask for src as an argument
def start(context, argv):
    if len(argv) != 2:
        raise ValueError(&amp;#39;Usage: -s &amp;quot;js_injector.py src&amp;quot;&amp;#39;)
    context.src_url = argv[1]


def response(context, flow):
    if flow.request.host in context.src_url:
        return # Make sure JS isn&amp;#39;t injected to itself
    with decoded(flow.response):  # Remove content encoding (gzip, ...)
        html = BeautifulSoup(flow.response.content)
        if html.body:
            script = html.new_tag(
                &amp;quot;script&amp;quot;,
                src=context.src_url,
                type=&amp;#39;application/javascript&amp;#39;)
            html.body.insert(0, script)
            flow.response.content = str(html)
            context.log(&amp;quot;Script injected.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Run the mitm proxy using:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ mitmdump -p 8888 -s &amp;quot;js_injector.py http://path/to/src.js&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note: that we are using &lt;code&gt;mitmdump&lt;/code&gt; istead of &lt;code&gt;mitmproxy&lt;/code&gt; for getting more detailed output.&lt;/p&gt;

&lt;p&gt;Note: If you want to inject javascript in an &lt;code&gt;https&lt;/code&gt; website, your javascript file must also be hosted on &lt;code&gt;https&lt;/code&gt; server.&lt;/p&gt;

&lt;p&gt;So far so good:
Now open the url in browser configured to use proxy server, you should see a broken website.&lt;/p&gt;

&lt;h3&gt;Any guess why the website is broken ?&lt;/h3&gt;

&lt;p&gt;The website is broken because in previous script our javascript is getting injected on every resource on page having &lt;code&gt;html.body&lt;/code&gt; which should not be the case.&lt;/p&gt;

&lt;p&gt;Replace:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;-- if html.body:
with
++ if html.body and (&amp;#39;text/html&amp;#39; in flow.response.headers[&amp;quot;content-type&amp;quot;][0]):
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to make sure only requests with &lt;code&gt;text/html&lt;/code&gt; in their header get js injection.&lt;/p&gt;

&lt;p&gt;Try to open the page again, it should now open fine with &lt;em&gt;almost&lt;/em&gt; all resources loading properly.&lt;/p&gt;

&lt;h3&gt;So why &lt;em&gt;almost&lt;/em&gt; all and not &lt;em&gt;ALL&lt;/em&gt; ? Try to figure it out and read further:&lt;/h3&gt;

&lt;p&gt;The reason is that an html page has multiple resources that can be of type text/html for example an iframe loading content from an external resource. But we want to inject our js only to the page that is opened by user at current time i.e the main page.&lt;/p&gt;

&lt;p&gt;Its not possible to do this without knowing what page is the main page and what are the elements within it. Therefore to overcome this situation we come up with a filter.&lt;/p&gt;

&lt;p&gt;filter.js&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;if(parent.document.URL!=document.location.href)
       throw new Error(&amp;quot;Not the main page&amp;quot;);

(function(e){e.setAttribute(&amp;quot;src&amp;quot;,&amp;quot;http://path/to/script.js&amp;quot;);
document.getElementsByTagName(&amp;quot;body&amp;quot;)[0].appendChild(e);})
(document.createElement(&amp;quot;script&amp;quot;));void(0);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So basically injecting this filter on proxy server side by replacing path/to/script.js with the path to actual js that we want to inject will do a client side verification to identify the main page and allow execution of js only on main page otherwise it will throw an error &amp;quot;Not the main page&amp;quot;.&lt;/p&gt;

&lt;p&gt;This solves our problem of js getting executed multiple times on a single page. Now the js we inject will execute only once but still on the proxy server side our js filter will get inject to all valid html resources i.e which have &lt;code&gt;text/html&lt;/code&gt; as their &lt;code&gt;content-type&lt;/code&gt; in header response.&lt;/p&gt;

&lt;h2&gt;Final Steps:&lt;/h2&gt;

&lt;p&gt;Inject JS filter on html resources:&lt;/p&gt;

&lt;p&gt;Final version of &lt;code&gt;js_injector.py&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;# Usage: mitmdump -s &amp;quot;js_injector.py src&amp;quot;
# (this script works best with --anticache)
from bs4 import BeautifulSoup
from libmproxy.protocol.http import decoded

# On start of proxy server ask for src as an argument
def start(context, argv):
    if len(argv) != 2:
        raise ValueError(&amp;#39;Usage: -s &amp;quot;js_injector.py src&amp;quot;&amp;#39;)
    context.src_url = argv[1]


def response(context, flow):
    with decoded(flow.response):  # Remove content encoding (gzip, ...)
        html = BeautifulSoup(flow.response.content)
        &amp;quot;&amp;quot;&amp;quot;
        # To Allow CORS
        if &amp;quot;Content-Security-Policy&amp;quot; in flow.response.headers:
            del flow.response.headers[&amp;quot;Content-Security-Policy&amp;quot;]
        &amp;quot;&amp;quot;&amp;quot;
        if html.body and (&amp;#39;text/html&amp;#39; in flow.response.headers[&amp;quot;content-type&amp;quot;][0]):
            script = html.new_tag(
                &amp;quot;script&amp;quot;,
                src=context.src_url)
            html.body.insert(0, script)
            flow.response.content = str(html)
            context.log(&amp;quot;******* Filter Injected *******&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ mitmdump -p 8888 -s &amp;quot;js_injector.py http://path/to/filter.js&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Final version of &lt;code&gt;filter.js&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;if(parent.document.URL!=document.location.href)
       throw new Error(&amp;quot;Not the main page&amp;quot;);

(function(e){e.setAttribute(&amp;quot;src&amp;quot;,&amp;quot;http://path/to/script.js&amp;quot;);
document.getElementsByTagName(&amp;quot;body&amp;quot;)[0].appendChild(e);})
(document.createElement(&amp;quot;script&amp;quot;));void(0);

console.log(&amp;quot;******* Script Injected *******&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Replace &lt;code&gt;/path/to/script.js&lt;/code&gt; in &lt;code&gt;filter.js&lt;/code&gt; with path to the script you want to actually inject.&lt;/p&gt;

&lt;p&gt;That&amp;#39;s it ! Now the JS you inject will be executed only one time per page i.e the actual page and not the resources in it.&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>Simple HTTPS Server In Python Using Self Signed Certs</title>
        <link>http://pankajmalhotra.com/Simple-HTTPS-Server-In-Python-Using-Self-Signed-Certs</link>
        <pubDate>Sun, 07 Jun 2015 20:54:20 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;So I came across a situation where I needed to boot up an https server to host some files and guess what its really very easy to do than what I imagined.&lt;/p&gt;

&lt;h2&gt;Generate self signed certificates using OpenSSL&lt;/h2&gt;

&lt;h3&gt;Generate your server key&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ openssl genrsa -des3 -out server.key 1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You will be prompted for a password for your key. Enter, confirm and continue.&lt;/p&gt;

&lt;h3&gt;Generate your Certificate Signing Request (CSR)&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ openssl req -new -key server.key -out server.csr
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You will be prompted again for your key password. Enter the one you created from step 1 above. You can then accept the defaults for all of the prompts you are presented with except the Common Name. This is key, and what makes the enhanced certificate validation happy. Since we are doing local development your common name will be &lt;code&gt;localhost&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Common Name (e.g. server FQDN or YOUR name) []:localhost
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Generate your Certificate&lt;/h3&gt;

&lt;p&gt;Lastly we need to create our certificate. Again, use your key password and you will be all set.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ openssl x509 -req -days 1024 -in server.csr -signkey server.key -out server.crt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Generate a pem file&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ cat server.crt server.key &amp;gt; server.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Python implementation of server:&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;#!/usr/bin/python

import BaseHTTPServer, SimpleHTTPServer
import ssl

httpd = BaseHTTPServer.HTTPServer((&amp;#39;localhost&amp;#39;, 4443), SimpleHTTPServer.SimpleHTTPRequestHandler)
httpd.socket = ssl.wrap_socket (httpd.socket, certfile=&amp;#39;/path/to/server.pem&amp;#39;, server_side=True)
httpd.serve_forever()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Run &lt;code&gt;$ python server.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Default host here is &lt;code&gt;localhost&lt;/code&gt; and port is &lt;code&gt;4443&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That&amp;#39;s it ! You have finally deployed an &lt;code&gt;https&lt;/code&gt; server.
Go to &lt;code&gt;https://localhost:4443&lt;/code&gt; and click &lt;code&gt;Advanced &amp;gt; Proceed to localhost(unsafe)&lt;/code&gt; to accept certificates and see the serverd files.&lt;/p&gt;

&lt;h3&gt;Sources:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.salesforce.com/blogs/developer-relations/2011/05/generating-valid-self-signed-certificates.html&quot;&gt;Generating valid self signed certificates for localhost development&lt;/a&gt;
&lt;br/&gt;
&lt;a href=&quot;http://www.piware.de/2011/01/creating-an-https-server-in-python/&quot;&gt;Creating an HTTPS server in Python&lt;/a&gt;&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>Add Two Numbers Without Using Arithmetic Operators</title>
        <link>http://pankajmalhotra.com/Add-Two-Numbers-Without-Using-Arithmetic-Operators</link>
        <pubDate>Mon, 04 May 2015 07:10:56 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;Write a function Add() that returns sum of two integers. The function should not use any of the arithmetic operators (+, ++, –, -, .. etc).&lt;/p&gt;

&lt;p&gt;Sum of two bits can be obtained by performing XOR (^) of the two bits. Carry bit can be obtained by performing AND (&amp;amp;) of two bits.
Above is simple Half Adder logic that can be used to add 2 single bits. We can extend this logic for integers. If x and y don’t have set bits at same position(s), then bitwise XOR (^) of x and y gives the sum of x and y. To incorporate common set bits also, bitwise AND (&amp;amp;) is used. Bitwise AND of x and y gives all carry bits. We calculate (x &amp;amp; y) &amp;lt;&amp;lt; 1 and add it to x ^ y to get the required result.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;#include&amp;lt;stdio.h&amp;gt;

int Add(int x, int y)
{
    // Iterate till there is no carry  
    while (y != 0)
    {
        // carry now contains common set bits of x and y
        int carry = x &amp;amp; y;  

        // Sum of bits of x and y where at least one of the bits is not set
        x = x ^ y; 

        // Carry is shifted by one so that adding it to x gives the required sum
        y = carry &amp;lt;&amp;lt; 1;
    }
    return x;
}

int main()
{
    printf(&amp;quot;%d&amp;quot;, Add(15, 32));
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Following is recursive implementation for the same approach.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;int Add(int x, int y)
{
    if (y == 0)
        return x;
    else
        return Add( x ^ y, (x &amp;amp; y) &amp;lt;&amp;lt; 1);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Credits : http://www.geeksforgeeks.org&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>New Algorithms and Coding Section</title>
        <link>http://pankajmalhotra.com/New-Algorithms-and-Coding-Section</link>
        <pubDate>Thu, 30 Apr 2015 14:02:15 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;Being heavily involved in Software Development and Automation from the last year, I didn&amp;#39;t get much time to do competitive programming. I used to read algorithms and research papers in free time and implement some of them or solve some interesting problem. So to share these codes today I added a new section &lt;a href=&quot;http://pankajmalhotra.com/coding&quot;&gt;coding&lt;/a&gt; to my site.&lt;/p&gt;

&lt;h2&gt;What will it contain ?&lt;/h2&gt;

&lt;p&gt;This new section will contain a very brief explanation of Algorithm or the problem and the code for implementing it. Most of the problems will be from &lt;a href=&quot;http://www.topcoder.com/&quot;&gt;TopCoder&lt;/a&gt; , &lt;a href=&quot;http://codeforces.com/&quot;&gt;CodeForces&lt;/a&gt; , &lt;a href=&quot;http://www.codechef.com/&quot;&gt;CodeChef&lt;/a&gt; , &lt;a href=&quot;https://www.hackerrank.com/&quot;&gt;HackerRank&lt;/a&gt; and &lt;a href=&quot;http://www.spoj.com/&quot;&gt;SPOJ&lt;/a&gt;. I will add some brain teasers and logic puzzles too. Since I am planning to add them frequently, so they will not be verbose.&lt;/p&gt;

&lt;p&gt;This section might also sometimes contain cool and fun scripts, So keep checking !&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>Deploying Projects On AWS Using ATLAS Workflow</title>
        <link>http://pankajmalhotra.com/Deploying-Projects-On-AWS-Using-ATLAS-Workflow</link>
        <pubDate>Sat, 18 Apr 2015 04:41:38 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;In the &lt;a href=&quot;http://pankajmalhotra.com/Infrastructure-Automation-Using-Terraform-Packer-Consul-Atlas/&quot;&gt;last&lt;/a&gt; post I talked about Infrastructure automation and gave a brief introduction about tools like Terraform, Packer, Consul and also talked about ATLAS and the workflow for deploying deploying infrastructure using it.&lt;/p&gt;

&lt;p&gt;Today I am gonna get into details on how to actually deploy projects like a LAMP server, Wordpress, Discourse, Docker on AWS using Atlas workflow and how to generate and associate RSA keys and security group to SSH into any of your machines.&lt;/p&gt;

&lt;p&gt;Most of the informtion is in linked guides for each of the projects, I&amp;#39;ll concentrate on main parts and things to pay special attention to.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://atlas.hashicorp.com/help/getting-started/getting-started-overview&quot;&gt;getting-started&lt;/a&gt; guide gives a very good idea about how things work and practically explains the complete ATLAS workflow with a very easy example.&lt;/p&gt;

&lt;h2&gt;Deploying Wordpress on AWS&lt;/h2&gt;

&lt;p&gt;Main thing to note on this tutorial is that you have to deploy database on a separate node since the aws instance gets rebuild everytime you push a new version of you application or make any configuration changes, so to preserve the application data its advised to deploy database on a separate node. We are using Consul here as a service discovery tool so that we get to get information about our application and databse server. So you don&amp;#39;t have to hardcode any ip address or any other other resource attached to a particular service.&lt;/p&gt;

&lt;p&gt;Since we are using Consul here, we will have to allow &lt;code&gt;udp&lt;/code&gt; traffic on the port which Consul is running. As mentioned consul uses a &lt;code&gt;gossip&lt;/code&gt; protocol to establish communication between services.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hashicorp/atlas-examples/tree/master/wordpress&quot;&gt;Link to tutorial&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Using Travis to push Application code to ATLAS&lt;/h2&gt;

&lt;p&gt;As the name suggests this is an example demonstarting how to push application code to ATLAS from Travis. Here we use &lt;a href=&quot;https://github.com/hashicorp/atlas-upload-cli&quot;&gt;ATLAS Upload CLI&lt;/a&gt;, a simple utility to push application code to Atlas.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hashicorp/atlas-examples/tree/master/TravisCI&quot;&gt;Link to tutorial&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;SSH in an AWS Instance&lt;/h2&gt;

&lt;p&gt;If you are setting up a production infrastructure, its advised not to allow any ports open or leave any authentication keys in aws instance but surely for debugging purpose you need to sometimes SSH in the instance you created using ATLAS workflow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hashicorp/atlas-examples/blob/master/AWS-SSH-Setup/ssh.md&quot;&gt;Link to tutorial&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Deploy Docker Container on AWS&lt;/h2&gt;

&lt;p&gt;Support for Docker provider is released in recent version of terraform &lt;code&gt;0.4&lt;/code&gt;, it still lacks some of requirements but the work is under progress and more features are expected to be released in upcoming versions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/bitgeeky/atlas-examples/tree/docker/Docker&quot;&gt;Link to tutorial&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Deploy Discourse on AWS&lt;/h2&gt;

&lt;p&gt;Discourse uses docker and does a lot of bootstraping to the instance before deploying the actual application code. So its advised to follow their official guide and treat the bootstraping part as a black box to avoid breaking anything.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/bitgeeky/atlas-examples/tree/discourse/Discourse&quot;&gt;Link to tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There are many other examples in official HashiCorp&amp;#39;s &lt;a href=&quot;https://github.com/hashicorp/atlas-examples&quot;&gt;ATLAS Examples&lt;/a&gt; respository on GitHub. I would recommend to go through some of them to get a good grip of how things work. &lt;/p&gt;

&lt;p&gt;If you think any project is missing feel free to raise an issue or You Can Add One Too !&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>Infrastructure Automation Using Terraform, Packer, Consul and ATLAS Workflow</title>
        <link>http://pankajmalhotra.com/Infrastructure-Automation-Using-Terraform-Packer-Consul-Atlas</link>
        <pubDate>Fri, 17 Apr 2015 03:35:19 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;I have been working as a software contractor for HashiCorp for over a month now. My main work is to write examples for HashiCorp &lt;a href=&quot;https://atlas.hashicorp.com/&quot;&gt;ATLAS&lt;/a&gt; which is an upcoming product from HashiCorp and is under tech review at the moment. During this time I got to work on tools like Terraform, Packer and Consul. These tools are all from HashiCorp. In this blog post I&amp;#39;ll describe briefly what each of these tools is used for and about the examples I wrote for Atlas.&lt;/p&gt;

&lt;h2&gt;What is Infrastructure Automation ?&lt;/h2&gt;

&lt;p&gt;As the name suggests its automating the process of creating vm&amp;#39;s, managing load balancers, security groups, ssh-keys, ip-address and all the other resources that are associated with the machine or container.
When the size of data centres is large and it consists of hundreds or thousands of nodes and servers its impossible to handle these resources manually. So you need an automation tool that would do the work for you and to preserve all the infrastructure as a code, configuration files to be more precise. Based on the infrastructure configuration and the commands you pass every thing in cloud can be controlled using the automation tool.&lt;/p&gt;

&lt;h2&gt;What is Terraform ?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt; is an Infrastructure automation tool used to store infrastructure as code. Its written in &lt;code&gt;go&lt;/code&gt; and has its own syntax for writing infrastructure configuration using &lt;code&gt;*.tf&lt;/code&gt; file. A sample terraform file looks like:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;provider &amp;quot;aws&amp;quot; {
    access_key = &amp;quot;ACCESS_KEY_HERE&amp;quot;
    secret_key = &amp;quot;SECRET_KEY_HERE&amp;quot;
    region = &amp;quot;us-east-1&amp;quot;
}

resource &amp;quot;aws_instance&amp;quot; &amp;quot;web&amp;quot; {
    instance_type = &amp;quot;t1.micro&amp;quot;
    ami = &amp;quot;ami-408c7f28&amp;quot;

    # This will create 1 instances
    count = 1
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So you can specify the priver AWS in this case and specify the resources you want to create on that provider like &lt;code&gt;aws_instance&lt;/code&gt; in this case. On running &lt;code&gt;$ terraform plan&lt;/code&gt; you can check the validity of your terraform configuration, it will give an error if there is any syntax issue, resource dependency issues and performs validation of resources, credentials etc but the actual output you get on running &lt;code&gt;$ terraform apply&lt;/code&gt; - which actually deploys these resources might differ from the output of &lt;code&gt;$ terraform plan&lt;/code&gt; in some cases.&lt;/p&gt;

&lt;p&gt;So running &lt;code&gt;$ terraform apply&lt;/code&gt; on this will create an aws instance with specified configuration. It actually creates a graph on backend to check for resource dependencies and you can get the graph using the commnd &lt;code&gt;$ terraform graph&lt;/code&gt;. If anything goes wrong you can run &lt;code&gt;$ terraform destroy&lt;/code&gt; to destroy the complete infrastructure.&lt;/p&gt;

&lt;p&gt;It comes with support from a lot of service providers AWS, Azure, Digital Ocean, Heroku, Google Cloud, Docker etc. All these service providers actually provide api&amp;#39;s to communicate with their services and terraform provides an abstarction layer over these api&amp;#39;s along with other very useful features.&lt;/p&gt;

&lt;p&gt;This is a very small example to just give an idea what terraform is all about, you should see the &lt;a href=&quot;https://www.terraform.io/docs/index.html&quot;&gt;official docmentation&lt;/a&gt; for many other features that it offers. It has developed a very good community also you can visit &lt;code&gt;#terraform-tool&lt;/code&gt; on freenode to ask any doubts etc.&lt;/p&gt;

&lt;h2&gt;What is Packer ?&lt;/h2&gt;

&lt;p&gt;Writing terraform files can get a lot complex sometimes when you need to install some packages on your machine or make a particular setup on the machine you just created. Its always advised to machine a machine image for whatever you want to do. In the machine image you can specify the packages you want to install on the machine, configuration you need to make before the machine actually gets created or scripts you want to execute after you create the machine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.packer.io/&quot;&gt;Packer&lt;/a&gt; allows you to deal with this task of creating machine images so that they can be deployed cross platform on multiple resource providers parallely. Its also written in &lt;code&gt;go&lt;/code&gt; and all the image configuration can be written in a simple &lt;code&gt;json&lt;/code&gt; format. A sample configuration looks like:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;{
  &amp;quot;variables&amp;quot;: {
    &amp;quot;aws_access_key&amp;quot;: &amp;quot;ACCESS_KEY_HERE&amp;quot;,
    &amp;quot;aws_secret_key&amp;quot;: &amp;quot;SECRET_KEY_HERE&amp;quot;
  },
  &amp;quot;builders&amp;quot;: [{
    &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;,
    &amp;quot;access_key&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;secret_key&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;region&amp;quot;: &amp;quot;us-east-1&amp;quot;,
    &amp;quot;source_ami&amp;quot;: &amp;quot;ami-9eaa1cf6&amp;quot;,
    &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;,
    &amp;quot;ssh_username&amp;quot;: &amp;quot;ubuntu&amp;quot;,
    &amp;quot;ami_name&amp;quot;: &amp;quot;packer-example &amp;quot;
  }]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Similar to &lt;code&gt;$ terraform plan&lt;/code&gt; you can validate the configuration using &lt;code&gt;$ packer validate example.json&lt;/code&gt; and the run &lt;code&gt;$ packer build example.json&lt;/code&gt; to actually create the machine image.&lt;/p&gt;

&lt;h2&gt;What is ATLAS ?&lt;/h2&gt;

&lt;p&gt;So now you have the machine images and your terraform deployment configurations. Lets say, you want to share your infrastructure with one of the team members or you need a place to host the machine images, develop your infrastructure further and then deploy it immediately. This is what ATLAS is designed for. It provides a platform to perform all these operations at a single place.&lt;/p&gt;

&lt;p&gt;I would describe &lt;a href=&quot;http://atlas.hashicorp.com/&quot;&gt;ATLAS&lt;/a&gt; as a utility to store and share your infrastrucutre. Like you can share and develop your application code on GitHub, ATLAS provides a platform to develop and share you infrastructure while providing instant deployment option.&lt;/p&gt;

&lt;p&gt;The workflow of ATLAS is like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create machine images using Packer and push them to Atlas.&lt;/li&gt;
&lt;li&gt;Push the application code to Atlas. You can use a vagrant box for this.&lt;/li&gt;
&lt;li&gt;Link application code to infrastructure images.&lt;/li&gt;
&lt;li&gt;Deploy Atlas artifacts using Terraform on cloud.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;What is Consul ?&lt;/h2&gt;

&lt;p&gt;Now that the infrastructure is up and running, how do we find that all nodes in our infrastrucure are behaving as expected and are up and running ? In another case say you deployed database on one of the nodes and application on the other one, now how does one nodes comes to know about other ? How can the application and database nodes communicate with each other ?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://consul.io/&quot;&gt;Consul&lt;/a&gt; provides a solution to these problems. Its basically a service discovery and health monitoring tool. Apart from this it can also be used as a Key/Value Storage and it can work across multi datacenters. So the nodes monitored by consul need not be present in the same datacenter. Its based on the &lt;code&gt;gossip&lt;/code&gt; protocol which itself utilises &lt;code&gt;udp&lt;/code&gt; for data transfer.&lt;/p&gt;

&lt;p&gt;So these are the interesting things I am working on. Apart from these I also worked on Docker containers and Vagrant boxes but those are quite common now.&lt;/p&gt;

&lt;h2&gt;What examples I wrote ?&lt;/h2&gt;

&lt;p&gt;I wrote a couple of examples on how to deploy Wordpress, link Travis, deploy Docker, Discourse and tutorials on getting SSH access to the machines in infractructure using the ATLAS workflow. I will give a brief introduction and link for each of these in the next blog post.&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>The Calm Before the Storm</title>
        <link>http://pankajmalhotra.com/The-calm-before-the-storm</link>
        <pubDate>Wed, 11 Mar 2015 01:50:20 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;I had gone &lt;em&gt;almost&lt;/em&gt; socially and publicaly silent for a couple of months. This was not by choice but I was busy experimenting with a lot of new things, had a few goals to achieve, learn new things and most of my time went in this. Still I have been wanting to write/blog about everything I tried, thing I learned, things I achieved and the challenging experience I had from past a few months but again time and the laziness was a big barrier.&lt;/p&gt;

&lt;p&gt;Its not that I am completely free now and had nothing else to do than write a blog post but as I say I was learning things and one of the things which I iterated learning was managing my time.
So yeah after managing my time I got a couple of moments in which I can share and write about things, I really have been longing to write about.&lt;/p&gt;

&lt;p&gt;I am planning to roll out the blog post about these things in a weekly post manner i.e one post per week. This will give me enough time to write a quality post and since the reader would be able to appreciate them.&lt;img src=&quot;/public/images/blog_posts_are_coming.jpg&quot; alt=&quot;Brace Yourselves, Blog Posts Are Coming&quot; style=&quot;float:left; margin-right:15px; margin-top:15px; width: 40%; height: 40%&quot; /&gt; 
These post range from a wide range of topics from my experimens with my diet, social life/facebook new feed, my achievement in these months from being and Intel Embedded Challenge Winner to working as a part time developer with HashiCorp and getting an Internship at BrowserStack while also discussing about the challenges I faced during this period.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h2&gt;Upcoming Blog Posts &lt;/h2&gt;

&lt;p&gt;These are some of the topics I have been wanting to write about and definitely as I write I will have more to share.&lt;/p&gt;

&lt;h2&gt;1. Being Technical Reviewer for PACKT Publishing&lt;/h2&gt;

&lt;p&gt;I got an offer from PACKT publishing to be a technical reviewer for one the their books on JQuery. I accpeted their offer but had to leave in between because of some reasons. There wasn&amp;#39;t any sort of contract. I would be blogging about my views on accepting the offer at first and reject it before even starting so that there isn&amp;#39;t any kind of loss.&lt;/p&gt;

&lt;h2&gt;2. Winning Intel India Embedded Challenge 2014&lt;/h2&gt;

&lt;p&gt;This was one of the best thing I had done in all the years I spent in college till now. I along with my two friends participated in Intel IEC 2014 and won the Jury&amp;#39;s popular choice award, a fat cash prize and a lot of fame. &lt;a href=&quot;https://iec2014.intel.com/winner2014.aspx&quot;&gt;Official Results&lt;/a&gt;.&lt;img src=&quot;/public/images/Intel_IEC.jpg&quot; alt=&quot;Intel IEC&quot; style=&quot;float:right; margin-left:15px; margin-top:15px; width: 50%; height: 50%&quot; /&gt;Thanks to the team, this would not have been possible without Aditya Jindal and Shubham Chhabra. We built an &amp;quot;Ambulatory EEG and seizure alert system&amp;quot;. Will be discussing more on this in the post and all about the competetion. One thing I would not miss out here is that this was the best Judgement I have ever seen. The Jury was of the most notable alumni of top most institutions and corporates and comprised of Judges with very diverse fields Mangament, Entrepreneurs, Engineers, Teachers, Proffesors and Doctors.&lt;/p&gt;

&lt;h2&gt;3. Algorithms and competetive programming&lt;/h2&gt;

&lt;p&gt;Honestly speaking I wasn&amp;#39;t much into Algorithms and competetive programming till I realised that things would be really tough if I would not practice enough Algorithms to crack interviews for Software Engineering Intern and yes from that point I started reading a lot of Algorithsm and would solve a few problems daily.
I gradually developed interest and saw its intersection with Software Development. Obviously, you would never be able to write a good software unless you know your Algorithms and Data Structures. This thing wasn&amp;#39;t actually too obvious to me initially.&lt;/p&gt;

&lt;h2&gt;4. Software Developer Contractor for &lt;a href=&quot;http://atlas.hashicorp.com/&quot;&gt;HashiCorp ATLAS&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;I am writing code for &lt;a href=&quot;https://hashicorp.com/&quot;&gt;HashiCorp&lt;/a&gt;, YES you heard it right ! This was something I have been wanting to do since when I started programming but I didn&amp;#39;t know my interest because I never knew what DevOps is and what Distributed Systems is.&lt;img src=&quot;/public/images/HashiCorp_Logo.png&quot; alt=&quot;HashiCorp Logo&quot; style=&quot;float:left; margin-right:15px; margin-top:15px; width: 20%; height: 20%&quot; /&gt;I always wanted to make things which could communicate in real time and send data across machines,  I developed a few multiplayer games and used Socket IO alot to make chat applications. But now I know what I want to do and I read a lot about distributed algorithms, protocols and networking.
I can&amp;#39;t write much on this because of a contract but I would be blogging about my adventures with ATLAS.&lt;/p&gt;

&lt;h2&gt;5. Summer Internship 2015 at &lt;a href=&quot;http://www.browserstack.com/&quot;&gt;BrowserStack&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/BrowserStack_Logo.png&quot; alt=&quot;BrowserStack Logo&quot; style=&quot;float:left; margin-right:15px; width: 20%; height: 20%&quot; /&gt;I will be working with BrowseStack as a summer Intern this year. I will blog about the reasons why I chose BrowserStack to work for.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h2&gt;Blog Renovation&lt;/h2&gt;

&lt;p&gt;For now I have initialised this Queue for Blog posts with these topics and hope to make each post on time.
Apart from this I am planning to restructure my Blog and have some more features, I chose Jekyll and github.io for hosting in a hurry when I started blogging, but this it definitely lacks SEO optimisations.&lt;/p&gt;

&lt;p&gt;Restructuring the site is a less priority, and would do when I have enough time to design and plan a good one.&lt;/p&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;Was working on some big things, had to forgo some minor things and didn&amp;#39;t get much time to blog. Would be blogging about most of those things in upcoming posts. &lt;/p&gt;

&lt;h2&gt;Keep Checking !&lt;/h2&gt;
</description>
      </item>
    
      <item>
        <title>ARP and Thunderbolt-Ethernet issues with OS X Mavericks</title>
        <link>http://pankajmalhotra.com/ARP-and-ethernet-issues-with-osx-mavericks</link>
        <pubDate>Sun, 17 Aug 2014 10:45:20 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;p&gt;Recently I got a new MacBook Pro(late 2013 model) 13.3 Retina display and everything was working like charm while I was using wifi to connect to internet and that pretty much should be because you expect everything perfect after spending that much amount on a machine.
After moving to my university where wifi connectivity is not present at every place I switched to Thunderbolt to Gigabit Ethernet to connect to internet and this was the moment when things seemed to be changing and I started getting troubles connecting to internet.&lt;/p&gt;

&lt;h2&gt;Problem&lt;/h2&gt;

&lt;p&gt;At the first time when I connected to internet it took about 15 min for internet to get stabilized and was continuously connecting and dissconnecting as if someone was continuously playing with the Ethernet cable, I ignored it and continued with my work assuming some problem with the VLAN used in university hostel and after some time the internet got dissconnected again automatically while working and came back after around 2 min.&lt;/p&gt;

&lt;p&gt;I tried to ping google from terminal and observed quite significant packet loss like this one:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;64 bytes from 74.125.236.128: icmp_seq=0 ttl=53 time=56.825 ms
64 bytes from 74.125.236.128: icmp_seq=1 ttl=53 time=56.839 ms
64 bytes from 74.125.236.128: icmp_seq=2 ttl=53 time=56.583 ms
64 bytes from 74.125.236.128: icmp_seq=3 ttl=53 time=56.770 ms
64 bytes from 74.125.236.128: icmp_seq=4 ttl=53 time=56.663 ms
64 bytes from 74.125.236.128: icmp_seq=5 ttl=53 time=56.894 ms
64 bytes from 74.125.236.128: icmp_seq=6 ttl=53 time=56.622 ms
64 bytes from 74.125.236.128: icmp_seq=7 ttl=53 time=58.007 ms
Request timeout for icmp_seq 8
Request timeout for icmp_seq 9
Request timeout for icmp_seq 10
Request timeout for icmp_seq 11
Request timeout for icmp_seq 12
64 bytes from 74.125.236.128: icmp_seq=13 ttl=53 time=57.267 ms
64 bytes from 74.125.236.128: icmp_seq=14 ttl=53 time=57.267 ms
64 bytes from 74.125.236.128: icmp_seq=15 ttl=53 time=57.267 ms
64 bytes from 74.125.236.128: icmp_seq=15 ttl=53 time=57.267 ms
64 bytes from 74.125.236.128: icmp_seq=16 ttl=53 time=56.713 ms
64 bytes from 74.125.236.128: icmp_seq=17 ttl=53 time=57.376 ms
64 bytes from 74.125.236.128: icmp_seq=18 ttl=53 time=57.811 ms
64 bytes from 74.125.236.128: icmp_seq=19 ttl=53 time=57.001 ms
64 bytes from 74.125.236.128: icmp_seq=20 ttl=53 time=56.869 ms&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This was the first problem - Packet loss for ping requests. &lt;/p&gt;

&lt;p&gt;It could still be tolerated but the most annoying part was I was not able to connect to internet after a reboot or sleep and had to wait for around 15-30 min everytime for the connection to get stabilized.
I checked the netork preferences at the this time by going to &lt;/p&gt;

&lt;p&gt;System Preference &amp;gt; Network and observed that machine could not even fetch the Ip address from DHCP server and the Thuderbot to ethernet connection was continuously flickering Green and Red light.&lt;/p&gt;

&lt;p&gt;I googled about the problem to find if its just my machine or other users are facing this problem and found some very useful sources of discussion:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://discussions.apple.com/thread/5483424?start=0&amp;amp;tstart=0&quot;&gt;Discussion on Apple Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.reddit.com/r/sysadmin/comments/1yc6n1/packet_losses_with_new_os_x_mavericks_make_sure&quot;&gt;Reddit Discussion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.macstadium.com/blog/osx-10-9-mavericks-bugs/&quot;&gt;MacStadum Blog providing updates on issue&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Which clearly indicate this issue with ARP caching by OS X Mavericks and problem with only some of the machines and not all.
So I was curious to check my arp output at this point of time and here is what it looked like:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
? (10.1.34.1) at 0:7:e:64:f9:3f on en4 ifscope [ethernet]
? (10.1.34.255) at ff:ff:ff:ff:ff:ff on en4 ifscope [ethernet]
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
? (10.1.34.1) at 0:7:e:64:f9:3f on en4 ifscope [ethernet]
? (10.1.34.255) at ff:ff:ff:ff:ff:ff on en4 ifscope [ethernet]
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
? (10.1.34.1) at 0:7:e:64:f9:3f on en4 ifscope [ethernet]
? (10.1.34.99) at 28:92:4a:43:c2:36 on en4 ifscope [ethernet]
? (10.1.34.157) at b8:88:e3:e2:aa:c7 on en4 ifscope [ethernet]
? (10.1.34.255) at ff:ff:ff:ff:ff:ff on en4 ifscope [ethernet]
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
? (10.1.34.1) at 0:7:e:64:f9:3f on en4 ifscope [ethernet]
? (10.1.34.255) at ff:ff:ff:ff:ff:ff on en4 ifscope [ethernet]
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
? (10.1.34.1) at 0:7:e:64:f9:3f on en4 ifscope [ethernet]
? (10.1.34.70) at b4:b5:2f:2f:c4:64 on en4 ifscope [ethernet]
? (10.1.34.255) at ff:ff:ff:ff:ff:ff on en4 ifscope [ethernet]
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a
Pankajs-MacBook-Pro:~ bitgeeky$ arp -a&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is at the moment when problem is lot worse.&lt;/p&gt;

&lt;p&gt;I tried a bunch of proposed solution proposed on various internet sites but nothing worked for me and then I came across a script which explains the problem and gives a solution that worked for many other users &lt;a href=&quot;https://github.com/MacMiniVault/Mac-Scripts/blob/master/unicastarp/unicastarp-README.md&quot;&gt;Disable Unicast ARP Cache Validation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As mentioned it disables Unicast ARP cache validation. But... unfortunately this also did not work for me :-(&lt;/p&gt;

&lt;h2&gt;Solution&lt;/h2&gt;

&lt;p&gt;The solution which is quite promising and worked for me is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Bind MAC address of networking interface(Thunderbolt to Gigabit ethernet in this case) to IP Address in DHCP configuration so that problem with ARP requests reduces quite significantly.&lt;/li&gt;
&lt;li&gt;After this it is also useful to remove all network interfaces from network preferences by clicking the &amp;quot;-&amp;quot; button shown in image and reboot. It is because I observed Thunderbolt-Bridge interfaring with the Thunderbolt-Ethernet and making it Fail in Assist-Me menu for network configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/public/images/network.png&quot; alt=&quot;Network Preferences OS X Mavericks&quot; style=&quot;float:left; margin-right:15px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This solution works fine for me and I am able to connect to internet using Thunderbolt to Gigabit Ethernet but at times I still see some packet loss which is almost insignificant.&lt;/p&gt;

&lt;p&gt;So there is no permanent solution as of now but I hope Apple to investigate more on this and fix this in their next release. Some people are really pushing them to fix the problem and I think most of updates about this issue can be found on &lt;a href=&quot;http://www.macstadium.com/blog/osx-10-9-mavericks-bugs/&quot;&gt;MacStadium Blog&lt;/a&gt; who are using Mac Mini&amp;#39;s in their data centeres, having the same problem with many of their machines and are in regular contact with the Apple Developers.&lt;/p&gt;

&lt;p&gt;I thought this piece of interrogation and ispection worth sharing with everyone so that you don&amp;#39;t waste time trying random hacks described on various internet websites if you have the same issue.&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>GSOC Project - Experience Summarized</title>
        <link>http://pankajmalhotra.com/Project-Wrap-Up</link>
        <pubDate>Fri, 01 Aug 2014 09:31:54 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>This was the last week of my GSOC project intern with Mozilla and here are a few things I would like to share about my project:
&lt;br/&gt;&lt;br/&gt;
&lt;h3&gt;Project Summary(What was actually done in the project ?)&lt;/h3&gt;

Project started with a target to write selenium end-to-end test for the &lt;a href=&quot;https://wiki.mozilla.org/QA/OneandDone&quot; target=_blank&gt;OneandDone&lt;/a&gt; project. It started with some reasearch about existing testing tools
pytest, pytest-django, pytest-mozwebqa, factoryboy etc.
&lt;br/&gt;
We finalized to use the existing testing model but with a change in the data population technique, this time we wanted not to rely on the data on the
website, instead we wanted to use the REST API to auto populate the database with the testing data before the test runs.
&lt;br/&gt;&lt;br/&gt;
The version of REST API at that time was not equiped to handle user related operations i.e creating, updating and deleting users, it was able to only
perform task related operations.
&lt;br/&gt;
So to start with I wrote the REST API for creating, updating and deleting users in database. This API was written using django-restframework and I must
say that it is a pretty good framework with a very good community support.
&lt;br/&gt;
The major challenge I faced was while working with nested serializers, it was not equiped to support the update operation at that time. I spent quite
some time talking to the restframework developers on their irc channel #restframework in freenode everyday and I must say that they were always helpful.
&lt;br/&gt;
Sometimes I also used to ask questions(which could not get answered on irc) on StackOverflow.
&lt;br/&gt;&lt;br/&gt;
After adding the API support for users I started writing the selenium webdriver tests which provide coverage for:
&lt;ol&gt;
    &lt;li&gt;Login as a new user.&lt;/li&gt;
    &lt;li&gt;Login as an existing user.&lt;/li&gt;
    &lt;li&gt;Test to Complete a task.&lt;/li&gt;
    &lt;li&gt;Test to abandon a task.&lt;/li&gt;
    &lt;li&gt;Test that existing user could update profile.&lt;/li&gt;
    &lt;li&gt;Test that existing user could delete profile.&lt;/li&gt;
&lt;/ol&gt;
&lt;br/&gt;
These tests used the selenium webdriver along with pytest-mozwebqa(which is a tool developed by the webqa team and consists of pytest + pytest-xdist and
is configured to give html reports of test runs for different environments, saucelabs and jenkins CI)
&lt;br/&gt;
All the Base Setup and API utilities to create/update/delete user data was also added along with the tests.
&lt;br/&gt;&lt;br/&gt;
After writing these tests I also updated the user REST API to also include data for User Profile and modified the tests accordingly.
&lt;br/&gt;&lt;br/&gt;
We had a release planned to 30 June for the website and some of the major P1 bugs were still left unassigned and my mentor Bob Silverberg(:bsilverberg)
was the only one working on them at that time. Since the selenium tests were a low priority compared to the P1 bugs marked for release I decided to help
my mentor and took 3 major P1 privacy bugs:
&lt;br/&gt;&lt;br/&gt;
&lt;ol&gt;
    &lt;li&gt;Add ability to delete profile.&lt;/li&gt;
    &lt;li&gt;Add username and privacy checkbox in user profile.&lt;/li&gt;
    &lt;li&gt;Create an interface to force existing users to sign the privacy policy or to delete profile.&lt;/li&gt;
&lt;/ol&gt;
&lt;br/&gt;

Fortunately I was able to fix all three of them in time and we had a great release on 30 June. I was really glad to see the features I added in the latest
release.
&lt;br/&gt;&lt;br/&gt;

The latest release brought a lot of changes to the website and the existing selenium tests were failing because of the recent changes. So my next job was
to fix the failing tests.
&lt;br/&gt;&lt;br/&gt;

Test related to User Profile were also added during this period:
&lt;br/&gt;
Test to verify duplicate usernames are not allowed
&lt;br/&gt;&lt;br/&gt;

There were also changes to the task model itself and the existing task API needed to be enhanced, so I spend some time enhancing the task API so that it
could be used in tests for creating tasks before test runs.
&lt;br/&gt;&lt;br/&gt;

After adding the API support it was time to write some task related tests and I started with some of them:
&lt;ol&gt;
    &lt;li&gt;Test Filter Tasks According to Estimated Time.&lt;/li&gt;
    &lt;li&gt;Test that one time tasks are marked completed on finishing.&lt;/li&gt;
    &lt;li&gt;Test that an already taken one time task cannot be taken by different user.&lt;/li&gt;
&lt;/ol&gt;
&lt;br/&gt;

This summarizes the major part of my project and there were also many minor tasks taken during this period. A link to all the bugs with exact links can be
found here:&lt;br/&gt;&lt;a href=&quot;https://bugzilla.mozilla.org/buglist.cgi?resolution=---&amp;resolution=FIXED&amp;resolution=INVALID&amp;resolution=WONTFIX&amp;emailtype1=exact&amp;query_format=advanced&amp;emailassigned_to1=1&amp;bug_status=NEW&amp;bug_status=ASSIGNED&amp;bug_status=RESOLVED&amp;bug_status=VERIFIED&amp;bug_status=CLOSED&amp;email1=mozpankaj1994%40gmail.com&quot; target=_blank&gt;Complete list of bugs taken up during project&lt;/a&gt;.
&lt;br/&gt;&lt;br/&gt;
&lt;h2&gt;Working Experience:&lt;/h2&gt;

First of all I would like to thank my mentor Bob Silverberg(:bsilverberg) for being such a great mentor and always providing guidance, support and quick reviews when required and to the whole Web QA team, members of QA and Webdev team for their support.
&lt;br/&gt;
It was the first time I was attending around 3 meetings in a week and making formal reports informing about my work and deciding about upcoming tasks and
schedule everything in advance. Initially things were a bit hard but gradually I observed myself growing and learning to collaborate with the team.
&lt;br/&gt;
This was a completely new experience, especially my project had both parts of Webdev and QA so I got to work with some leads from both teams and was an
integral part of team working on the OneandDone project.
&lt;br/&gt;
By the end I also found myself helping other contributors, giving code reviews and merging pull requests.
&lt;br/&gt;&lt;br/&gt;

This sums up my whole Google Summer of Code Project 2014 and my experience working with the awesome teams at Mozilla and I look forward to continue
as a regular contributor and take up internship at Mozilla next year.
</description>
      </item>
    
      <item>
        <title>REST API using Django REST Framework</title>
        <link>http://pankajmalhotra.com/REST-API-Using-DRF</link>
        <pubDate>Fri, 16 May 2014 09:03:05 +0000</pubDate>
        <author>Pankaj Malhotra</author>
        <description>&lt;div dir=&quot;ltr&quot; style=&quot;text-align: left;&quot; trbidi=&quot;on&quot;&gt;Last week we were pretty much concerned about what to use and what not ?&lt;br /&gt;and what changes we shall introduce in our model of Testing ?&lt;br /&gt;&lt;br /&gt;So may be if you have curiosity about what we used and what we decided, This post is for you !&lt;br /&gt;&lt;br /&gt;Overall Scenario last week: We wanted to use FactoryBoy for database object modelling and reliable data for tests.&lt;br /&gt;So using FactoryBoy meant that we had to keep our Functional Tests with the application itself.&lt;br /&gt;This was the change we decided to have and pretty much kicked of the work and &lt;a href=&quot;https://github.com/mozilla/oneanddone/pull/111&quot; target=&quot;_blank&quot;&gt;initialized the test objects in application repository&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;So what next ?&lt;br /&gt;In our old testing model we ran the Functional tests on staging/production application already running. So now the challenge was:&lt;br /&gt;How will we trigger the server ?&lt;br /&gt;&lt;br /&gt;One of the solution which instantly came to mind was using &lt;a href=&quot;https://docs.djangoproject.com/en/1.4/topics/testing/#django.test.LiveServerTestCase&quot; target=&quot;_blank&quot;&gt;LiveServerTestCase&amp;nbsp;&lt;/a&gt;&lt;br /&gt;So things were *almost* finalized !!&lt;br /&gt;&lt;br /&gt;But things come to mind as we start working, and this time it was not something which could be ignored.&lt;br /&gt;Man, we would not be able to run the tests on Staging and Production or in general any other remotely hosted clone of OneAndDone, Also running the tests locally would mean setting up of a local instance of OneAndDone. This was what came to mind as soon as I began writing the first test and it was for sure not what we wanted, because&amp;nbsp; of mainly two reasons:&lt;br /&gt;&lt;br /&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Why will a contributor developing a test case want to set up the application locally ? We definitely don&#39;t want to assume our contributors to be Application developers. So this was a hindrance for contributors.&lt;/li&gt;&lt;li&gt;We wanted to run the tests on staging and production because it gives us a better idea of current state of Application.&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;So we decided to give it a rethought.&lt;br /&gt;&lt;br /&gt;After many discussions with other team members it settled down to two options:&lt;br /&gt;&lt;ol style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Keep the tests local for better data population and faster test runs.&lt;/li&gt;&lt;li&gt;Have the tests in a separate repository making it easy for contributors to work on and to make us able to run the tests on stage/prod.&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;Finally we decided to go with the second one because of the above mentioned limitations of first, or the benefits of second overpowered the benefits of first. We realized our current model to be more advance than most of the other methods discovered.&lt;br /&gt;&lt;br /&gt;The main changes we decided to have in these tests that make them better and advance than the tests for most of the other Web QA projects are:&lt;br /&gt;&lt;br /&gt;&lt;ul style=&quot;text-align: left;&quot;&gt;&lt;li&gt;Develop and Use a REST API which interacts with database objects remotely. So it helps us to create/delete the test data. For developing this API we decided to use &lt;a href=&quot;http://www.django-rest-framework.org/&quot; target=&quot;_blank&quot;&gt;Django REST framework&lt;/a&gt; over &lt;a href=&quot;http://tastypieapi.org/&quot; target=&quot;_blank&quot;&gt;tastypie&lt;/a&gt; for some reasons based on prior experiences of developers.&lt;/li&gt;&lt;li&gt;We decided to use &lt;a href=&quot;http://pytest.org/latest/fixture.html&quot; target=&quot;_blank&quot;&gt;pytest fixtures&lt;/a&gt; instead of SetUp and TearDown methods which will allow creation/deletion of mock objects using the REST API.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;We again kicked off the work by initializing the &lt;a href=&quot;https://github.com/mozilla/oneanddone-tests/pull/1&quot; target=&quot;_blank&quot;&gt;Page Objects and Base Class&lt;/a&gt; and opened some Test Case bugs.&lt;br /&gt;Now I am looking forward to developing the REST API and start writing the tests next week.&lt;/div&gt;
</description>
      </item>
    
  </channel>
</rss>
